Explanation of the way in which A\-W\-E\-M\-C scrapes metadata for media files.

\section*{The Metadata\-Scraper interface}

There are three steps to all metadata scraping\-:


\begin{DoxyEnumerate}
\item Creating the scraper object\-: this happens at launch.
\item Preparing the scraper\-: this allocates memory to be used by the scraper and sets settings.
\item Scraping for data\-: the data is scraped using whatever method your class defines.
\item Deactivating the scraper\-: the objects allocated in step 2 are now deleted.
\end{DoxyEnumerate}

\subsection*{How to\-: Create Internal Metadata Scrapers using Plugins}

Essentially, you need to implement the {\ttfamily Metadata\-Scraper} interface, which is designed to be used as a plugin. Read the \href{http://qt-project.org/doc/qt-4.8/plugins-howto.html}{\tt how-\/to for Qt plugins} to learn more. You'll need to use \href{http://jsoncpp.sourceforge.net/>}{\tt Json\-Cpp} to write the settings file out.

In addition to creating the plugin, you must make a J\-S\-O\-N settings file. The only required entries in this file are\-: \begin{DoxyVerb}"name": <name of the scraper as it should appear to the user>,
"plugin": <the plugin library file>
\end{DoxyVerb}


You can include other settings in this file if you like; it should automatically be configurable in the main settings pane.

\subsection*{How To\-: Create External Metadata Scrapers using J\-S\-O\-N Files}

Before reading this how-\/to, here is some information you should know\-:


\begin{DoxyItemize}
\item The general format of a J\-S\-O\-N document (see \href{http://www.json.org/}{\tt http\-://www.\-json.\-org/} for more details).
\item The kinds of metadata that A\-W\-E\-M\-C accepts (see \href{../type/README.md>}{\tt Media Types} for details).
\item The way metadata is stored in a media file's J\-S\-O\-N preference file (see \href{../items/README.md>}{\tt Media Items} for details).
\item Qt regular expressions (see \href{http://qt-project.org/doc/qt-5/qregexp.html>}{\tt Q\-Reg\-Exp} for more details).
\end{DoxyItemize}

Note that, although comments are not officially supported by the J\-S\-O\-N file format, they are supported by A\-W\-E\-M\-C's J\-S\-O\-N reader, \href{http://jsoncpp.sourceforge.net/>}{\tt Json\-Cpp}. Standard {\ttfamily // comment} lines work.

\subsubsection*{Introduction to the idea behind J\-S\-O\-N-\/based scrapers}

The central theory behind these so-\/called {\itshape external scrapers} is that all metadata scrapers do basically the same thing\-: retrieve data from formatted files. Sometimes these are X\-M\-L files (e.\-g. .nfo files from X\-B\-M\-C) and sometimes they are web pages (e.\-g. \href{http://www.themoviedb.org/}{\tt http\-://www.\-themoviedb.\-org/}). J\-S\-O\-N-\/based scrapers in A\-W\-E\-M\-C make no distinction between documents hosted on the internet and locally; they are all just text files that the scraper must read to get information about the media. Thus all you have to do to get a piece of information from a file is\-:


\begin{DoxyEnumerate}
\item Get the file to read from.
\item Look for a specific section of text in that file.
\item Take parts of that section of text and assign those values to metadata properties.
\end{DoxyEnumerate}

The implentation if these so-\/called {\itshape procedures} is based on regular expressions and backreferences. The type of regex used is \href{http://qt-project.org/doc/qt-5/qregexp.html>}{\tt Q\-Reg\-Exp}, while the format of the string where backreferences are placed is defined by the {\ttfamily fmt} parameter for the C++ Standard Library's \href{http://www.cplusplus.com/reference/regex/match_replace/}{\tt backref replacement function}.

\subsubsection*{A template for a scraper}

Here is a template for a scraper\-: \begin{DoxyVerb}{
    // The user picks the scraper based on this name
    "name": <name of your scraper>,

    // This scraper is available to scrape metadata for this type of media item
    "type": <the type of media files>,

    // If a single file can have multiple items in it, set this to true.
    // The procedures below will be repeated for every file name match if true,
    // so make sure your filename regex is strict
    "multiple items per file": <true or false>

    // This helps you get information to use, like the name of the media item
    "filename": <regex to get backreferences from the media file path>,

    // If this type is hierarchical, the metadata should be inherited from the parent folder
    "inherited metadata": {
        <this type's property>: <parent type's property>
    }

    // List of procedures to run using the backreferences from "filename"
    "procedures": [
        {
            // Should this be repeated for every match or just one?
            "repeat": <true or false>,

            // Should the user be prompted to choose which match (or matches) to use?
            // Empty string means that the user should not be prompted
            "ask user": <string to prompt user with>,

            // The file to look in; backrefs are replaced
            "look in file": <formatted string with backrefs>,

            // The regex to search for in the above file; backrefs are replaced
            "for": <formatted regex with backrefs>,

            // Set a bunch of properties to backreferences from "for"
            "set properties": {
                <prop>: <value with backrefs>,
                <prop>: <value with backrefs>,
                // More properties...
            }

            // Run a list of sub-procedures using the backreferences from "for"
            "procedures": [...]
        },
        // More procedures...
    ]

    // For properties that contain file paths or are arrays of file paths, sometimes 
    // you want to import the files.
    // This should always be defined to import every file (except the media file);
    // The user can select settings at the launching of the scraper to decide if
    // this should use links to the files or copy the files.
    // you don't get to choose the new names or the location; AWEMC does that for you
    "copy": [
        <prop with file-path value>,
        <prop with file-path value>,
        // More properties...   
    ]

    // These are for files that should always be imported, e.g. internet files
    "force copy": [
        <prop with file-path value>
        // More properties...
    ]
}
\end{DoxyVerb}


Those of you who have read the \href{../type/README.md>}{\tt Media Types explanation} know that some metadata properties are actually objects that contain other metadata properties. In order to access a property held inside of an object, you must use the {\ttfamily .} operator, much like you would in C++. So to set the \char`\"{}default\char`\"{} property of the \char`\"{}icons\char`\"{} object, I would use\-:

\begin{DoxyVerb}"icons.default": <value>
\end{DoxyVerb}


\subsubsection*{Example scrapers}

There are plenty of example scrapers for you to look at in the {\ttfamily scrapers/json} directory in A\-W\-E\-M\-C's root folder. Specifically, {\ttfamily themoviedb.\-json} and {\ttfamily thetvdb.\-json} are included with A\-W\-E\-M\-C and well commented, so if you need a place to start read up on those.

\subsubsection*{Testing your scraper}

If you use the debug build of A\-W\-E\-M\-C you can see if your scraper J\-S\-O\-N is valid in the command line output. You will also be able to see each individual property being set. You should test your scraper thoroughly in this way before recommending it to others. 