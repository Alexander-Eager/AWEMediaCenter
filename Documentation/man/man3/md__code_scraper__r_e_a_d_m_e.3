.TH "md__code_scraper__r_e_a_d_m_e" 3 "Thu Apr 17 2014" "Version 0.1" "AWE Media Center" \" -*- nroff -*-
.ad l
.nh
.SH NAME
md__code_scraper__r_e_a_d_m_e \- Metadata Scraping 
Explanation of the way in which AWEMC scrapes metadata for media files\&.
.PP
.SH "The \fBAWEMetadataScraper\fP interface"
.PP
.PP
There are three steps to all metadata scraping:
.PP
.IP "1." 4
Creating the scraper object: this happens at launch\&.
.IP "2." 4
Preparing the scraper: this allocates memory to be used by the scraper and sets settings\&.
.IP "3." 4
Scraping for data: the data is scraped using whatever method your class defines\&.
.IP "4." 4
Deactivating the scraper: the objects allocated in step 2 are now deleted\&.
.PP
.PP
.SS "How to: Create Internal Metadata Scrapers using Plugins"
.PP
Essentially, you need to implement the \fC\fBAWEMetadataScraper\fP\fP interface, which is designed to be used as a plugin\&. Read the \fChow-to for Qt plugins\fP to learn more\&. You'll need to use \fCjsoncpp\fP to write the settings file out\&.
.PP
In addition to creating the plugin, you must make a JSON settings file\&. The only required entries in this file are:
.PP
``` 'name': <name of='' the='' scraper='' as='' it='' should='' appear='' to='' the='' user>=''>, 'plugin': <the plugin='' library='' file>=''> ```
.PP
You can include other settings in this file if you like; it should automatically be configurable in the main settings pane\&.
.PP
.SS "How To: Create External Metadata Scrapers using JSON Files"
.PP
Before reading this how-to, here is some information you should know:
.PP
.IP "\(bu" 2
The general format of a JSON document (see http://www.json.org/ for more details)\&.
.IP "\(bu" 2
The kinds of metadata that AWEMC accepts (see \&.\&./type/README\&.md 'Media Types' for details)\&.
.IP "\(bu" 2
The way metadata is stored in a media file's JSON preference file (see \&.\&./items/files/README\&.md 'Media Files' for details)\&.
.IP "\(bu" 2
ECMAScript regular expressions (see \fCthis online documentation\fP for details)\&.
.PP
.PP
Note that, although comments are not officially supported by the JSON file format, they are supported by AWEMC's JSON reader, \fCjsoncpp\fP\&. Standard \fC// comment\fP lines work\&.
.PP
.SS "Introduction to the idea behind JSON-based scrapers"
.PP
The central theory behind these so-called \fIexternal scrapers\fP is that all metadata scrapers do basically the same thing: retrieve data from formatted files\&. Sometimes these are XML files (e\&.g\&. \&.nfo files from XBMC) and sometimes they are web pages (e\&.g\&. http://www.themoviedb.org/)\&. JSON-based scrapers in AWEMC make no distinction between documents hosted on the internet and locally; they are all just text files that the scraper must read to get information about the media\&. Thus all you have to do to get a piece of information from a file is:
.PP
.IP "1." 4
Get the file to read from\&.
.IP "2." 4
Look for a specific section of text in that file\&.
.IP "3." 4
Take parts of that section of text and assign those values to metadata properties\&.
.PP
.PP
The implentation if these so-called \fIprocedures\fP is based on regular expressions and backreferences\&. The type of regex used is \fCECMAScript\fP, while the format of the string where backreferences are placed is defined by the \fCfmt\fP parameter for the C++ Standard Library's \fCbackref replacement function\fP\&.
.PP
.SS "A template for a scraper"
.PP
Here is a template for a scraper:
.PP
``` { // The user picks the scraper based on this name 'name': <name of='' your='' scraper>=''>,
.PP
// This scraper is available to scrape metadata for this type of media item 'type': <the type='' of='' media='' files>=''>,
.PP
// This helps you get information to use, like the name of the media item 'filename': <regex to='' get='' backreferences='' from='' the='' media='' file='' path>=''>,
.PP
// List of procedures to run using the backreferences from 'filename' 'procedures': [ { // Should this be repeated for every match or just one? 'repeat': <true or='' false>=''>,
.PP
// Should the user be prompted to choose which match (or matches) to use? 'ask user': <true or='' false>=''>,
.PP
// The file to look in; backrefs are replaced 'look in file': <formatted string='' with='' backrefs>=''>,
.PP
// The regex to search for in the above file; backrefs are replaced 'for': <formatted regex='' with='' backrefs>=''>,
.PP
// Set a bunch of properties to backreferences from 'for' <prop>: 
.PP
, <prop>: 
.PP
, // More properties\&.\&.\&.
.PP
// Run a list of sub-procedures using the backreferences from 'for' 'procedures': [\&.\&.\&.] }, // More procedures\&.\&.\&. ]
.PP
// For properties that contain file paths or are arrays of file paths, sometimes // you want to import the files\&. // This should always be defined to import every file (except the media file); // The user can select settings at the launching of the scraper to decide if // this should use links to the files or copy the files\&. // you don't get to choose the new names or the location; AWEMC does that for you 'copy': [ <prop with='' file-path='' value>=''>, <prop with='' file-path='' value>=''>, // More properties\&.\&.\&. ]
.PP
// These are for files that should always be imported, e\&.g\&. internet files 'force copy': [ <prop with='' file-path='' value>=''> // More properties\&.\&.\&. ] } ```
.PP
Those of you who have read the \&.\&./type/README\&.md 'Media Types explanation' know that some metadata properties are actually objects that contain other metadata properties\&. In order to access a property held inside of an object, you must use the \fC\&.\fP operator, much like you would in C++\&. So to set the 'default' property of the 'icons' object, I would use:
.PP
``` 'icons\&.default': 
.PP
```
.PP
.SS "Example scrapers"
.PP
There are plenty of example scrapers for you to look at in the \fCscrapers/json\fP directory in AWEMC's root folder\&. Specifically, \fCthemoviedb\&.json\fP and \fCthetvdb\&.json\fP are included with AWEMC and well commented, so if you need a place to start read up on those\&.
.PP
.SS "Testing your scraper"
.PP
If you use the debug build of AWEMC you can see if your scraper JSON is valid in the command line output\&. You will also be able to see each individual property being set\&. You should test your scraper thoroughly in this way before recommending it to others\&. 
